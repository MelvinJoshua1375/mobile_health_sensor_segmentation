# -*- coding: utf-8 -*-
"""Sensor Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Ye0yPJOuTiSfVP8XpwrMuCIvHN7QV53
"""

# Data Manipulation and Visualization
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# PySpark
from pyspark.sql import SparkSession
from pyspark.sql.functions import asc, count, col, mean
from pyspark.ml.feature import VectorAssembler, MinMaxScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

# Initialize Spark Session
spark = SparkSession.builder.appName("MobileHealthSegmentation").getOrCreate()

# Load the dataset
df = spark.read.csv("mobile_health.csv", header=True, inferSchema=True)
df.show(3)

"""# **EDA**"""

df.printSchema()

# Computes summary statistics
df.describe().show()

# Count missing values per column
missing_values = df.select([sum(col(c).isNull().cast("int")).alias(c) for c in df.columns])
missing_values.show()

df = df.dropDuplicates() # No duplicate values present
df.select('Subject').distinct().orderBy(asc("Subject")).show() # There are 10 Subjects

activity_counts = df.groupBy("Activity").agg(count("*").alias("Count"))
activity_counts.orderBy(asc("Activity")).show()

# Returns columns of dataframe
print(f"Features: {df.columns}")

# Counts the number of rows in dataframe
print(f"There are {df.count()} Rows and {len(df.columns)} Columns.")

df_pd = df.toPandas()
# Visualizing the Distribution of Activities
plt.figure(figsize=(10, 6))
sns.countplot(x='Activity', data=df_pd)
plt.title('Distribution of Activities')
plt.xticks(rotation=45, ha='right')
plt.show()

# Box Plots for Sensor Readings by Activity:
for sensor in ['alx', 'aly', 'alz', 'glx', 'gly', 'glz']:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='Activity', y=sensor, data=df_pd)
    plt.title(f'Distribution of {sensor} by Activity')
    plt.xticks(rotation=45, ha='right')
    plt.show()

def handle_outliers_iqr(df, column):
    """Handles outliers in a column using the IQR method."""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Replace outliers with NaN
    df.loc[(df[column] < lower_bound) | (df[column] > upper_bound), column] = np.nan

    return df

# Apply outlier handling to sensor columns
for sensor in ['alx', 'aly', 'alz', 'glx', 'gly', 'glz']:
    df_pd = handle_outliers_iqr(df_pd, sensor)

# Impute missing values using linear interpolation
for sensor in ['alx', 'aly', 'alz', 'glx', 'gly', 'glz']:
    df_pd[sensor] = df_pd[sensor].interpolate(method='linear', limit_direction='both')

# 'df_pd' has outliers handled and missing values imputed.

# Box Plots for Sensor Readings by Activity:
for sensor in ['alx', 'aly', 'alz', 'glx', 'gly', 'glz']:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='Activity', y=sensor, data=df_pd)
    plt.title(f'Distribution of {sensor} by Activity')
    plt.xticks(rotation=45, ha='right')
    plt.show()

# Select all columns except 'subject' and 'Activity'
features = [col for col in df_pd.columns if col not in ['subject', 'Activity']]

# Calculate the correlation matrix
corr_matrix = df_pd[features].corr()

# Plot the heatmap
plt.figure(figsize=(12, 10))  # Adjust figure size as needed
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Sensor Readings')
plt.show()

"""# **Data Modelling**"""

# Select only sensor data for clustering
feature_cols = ["alx", "aly", "alz", "glx", "gly", "glz", "arx", "ary", "arz", "grx", "gry", "grz"]
df_selected = df.select(feature_cols)

# Show the modified dataframe
df_selected.show(3)

# Select only sensor data (for pure clustering)
feature_cols_only_sensors = ["alx", "aly", "alz", "glx", "gly", "glz", "arx", "ary", "arz", "grx", "gry", "grz"]
df_sensors = df.select(feature_cols_only_sensors)

# Select sensor data + Activity (for comparison)
feature_cols_with_activity = feature_cols_only_sensors + ["Activity"]
df_with_activity = df.select(feature_cols_with_activity)

# Show both versions
print("Sensor Data Only:")
df_sensors.show(3)

print("Sensor Data + Activity:")
df_with_activity.show(3)

# Assemble features into a vector
assembler = VectorAssembler(inputCols=feature_cols_only_sensors, outputCol="features")
df_sensors_vector = assembler.transform(df_sensors)
df_with_activity_vector = assembler.transform(df_with_activity)

# Apply Min-Max Scaling
scaler = MinMaxScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(df_sensors_vector)
df_sensors_scaled = scaler_model.transform(df_sensors_vector)
df_with_activity_scaled = scaler_model.transform(df_with_activity_vector)

# Show scaled data
df_sensors_scaled.select("scaled_features").show(3, truncate=False)
df_with_activity_scaled.select("scaled_features", "Activity").show(3, truncate=False)

# Set number of clusters (initial guess, we will tune this later)
k = 3

# Train K-Means on sensor data only
kmeans_sensors = KMeans(featuresCol="scaled_features", k=k, seed=42)
model_sensors = kmeans_sensors.fit(df_sensors_scaled)
df_clusters_sensors = model_sensors.transform(df_sensors_scaled)

# Train K-Means on sensor data + Activity
kmeans_activity = KMeans(featuresCol="scaled_features", k=k, seed=42)
model_activity = kmeans_activity.fit(df_with_activity_scaled)
df_clusters_activity = model_activity.transform(df_with_activity_scaled)

# Show cluster assignments
print("Clusters without Activity:")
df_clusters_sensors.select("scaled_features", "prediction").show(5, truncate=False)

print("Clusters with Activity:")
df_clusters_activity.select("scaled_features", "prediction", "Activity").show(5, truncate=False)

wcss = []
k_values = list(range(2, 10))  # Testing k from 2 to 9

for k in k_values:
    kmeans = KMeans(featuresCol="scaled_features", k=k, seed=42)
    model = kmeans.fit(df_sensors_scaled)  # Using sensor data only
    wcss.append(model.summary.trainingCost)  # WCSS

# Plot the Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(k_values, wcss, marker="o", linestyle="-", color="b")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("WCSS")
plt.title("Elbow Method for Optimal k")
plt.show()

# Clustering Without Activity
kmeans_opt = KMeans(featuresCol="scaled_features", k=5, seed=42)
model_opt = kmeans_opt.fit(df_sensors_scaled)
df_clustered_opt = model_opt.transform(df_sensors_scaled)
df_clustered_opt.select("scaled_features", "prediction").show(5)

# Clustering With Activity
kmeans_opt_activity = KMeans(featuresCol="scaled_features", k=5, seed=42)
model_opt_activity = kmeans_opt_activity.fit(df_with_activity_scaled)
df_clustered_opt_activity = model_opt_activity.transform(df_with_activity_scaled)
df_clustered_opt_activity.select("scaled_features", "prediction", "Activity").show(5)

df_clustered_opt.groupBy("prediction").count().orderBy("prediction").show()

df_clustered_opt_activity.groupBy("prediction").count().orderBy("prediction").show()

# Initialize ClusteringEvaluator with Silhouette Score using Euclidean Distance
evaluator = ClusteringEvaluator(featuresCol="scaled_features", predictionCol="prediction", metricName="silhouette", distanceMeasure="squaredEuclidean")

# Compute Silhouette Score
silhouette_score = evaluator.evaluate(df_clustered_opt)
print(f"Silhouette Score: {silhouette_score:.2f}")

"""# **Model Deployment**"""

model_opt.save("kmeans_model")

df_clustered_opt.write.mode("overwrite").parquet("clustered_data.parquet")

from pyspark.ml.clustering import KMeansModel

loaded_model = KMeansModel.load("kmeans_model")

# !pip install flask pyngrok pandas pyarrow

from flask import Flask, render_template, jsonify
from pyngrok import ngrok
import pandas as pd
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
import os

app = Flask(__name__)

# Initialize Spark
spark = SparkSession.builder.appName("MobileHealthDeployment").getOrCreate()

# Load clustered data
clustered_df = spark.read.parquet("clustered_data.parquet")

# Compute cluster distribution
cluster_distribution = clustered_df.groupBy("prediction").count().toPandas().sort_values(by="prediction")

# Compute feature averages per cluster
feature_columns = [col for col in clustered_df.columns if col not in ["prediction", "features", "scaled_features"]]
cluster_summary = clustered_df.groupBy("prediction").agg(*[F.mean(col).alias(col) for col in feature_columns]).toPandas().sort_values(by="prediction")

@app.route('/')
def home():
    return render_template("index.html", clusters=cluster_distribution.to_dict(orient="records"), summary=cluster_summary.to_dict(orient="records"))

# Create templates folder if it doesn't exist
if not os.path.exists("templates"):
    os.makedirs("templates")

# Write HTML template
html_content = '''
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mobile Health Project Deployment</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body class="container mt-4">
    <h1 class="text-center">Mobile Health Sensor Segmentation</h1>
    <h2 class="mt-4">Cluster Distribution</h2>
    <table class="table table-bordered">
        <thead>
            <tr><th>Cluster</th><th>Count</th></tr>
        </thead>
        <tbody>
            {% for cluster in clusters %}
            <tr>
                <td>{{ cluster.prediction }}</td>
                <td>{{ cluster.count }}</td>
            </tr>
            {% endfor %}
        </tbody>
    </table>
    <h2 class="mt-4">Feature Averages per Cluster</h2>
    <table class="table table-bordered">
        <thead>
            <tr>
                <th>Cluster</th>
                {% for col in summary[0].keys() if col != 'prediction' %}
                <th>{{ col }}</th>
                {% endfor %}
            </tr>
        </thead>
        <tbody>
            {% for row in summary %}
            <tr>
                <td>{{ row.prediction }}</td>
                {% for col in row.keys() if col != 'prediction' %}
                <td>{{ row[col] | round(2) }}</td>
                {% endfor %}
            </tr>
            {% endfor %}
        </tbody>
    </table>
</body>
</html>
'''

with open("templates/index.html", "w") as f:
    f.write(html_content)

# Run the Flask app using ngrok
public_url = ngrok.connect(5000).public_url
print(f"Public URL: {public_url}")
app.run(port=5000)

# !ngrok authtoken 2uiCEWzFe4UYLLa29kE8cWzEpaA_6YXCEd5DdHByahjCRCgg5